{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Preprocessing\n",
    "\n",
    "### Cleaning + Estimating volatility\n",
    "- Ensuring validity of datapoints \n",
    "- Cleaning out deviations | Isolated points | etc ...\n",
    "- Estimating Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n"
     ]
    }
   ],
   "source": [
    "from preprocess_td import preprocess_tick_data\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    }
   ],
   "source": [
    "# Would be lovely to estimate parameters of function\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['return', 'SYMBOL'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSYMBOL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwavelet_vol\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolatility\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALUE\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['return', 'SYMBOL'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Noise Injection \n",
    "\n",
    "Suppose that, the latent log-price Xt is an Ito-semimartingale of the form \n",
    "\n",
    "$dX_t = b_t dt + \\sigma_t dW_t + dJ_t,$  \n",
    "$d\\sigma_t = \\tilde{b}_t dt + \\tilde{\\sigma}^{(1)}_t dW_t + \\tilde{\\sigma}^{(2)}_t d\\tilde{W}_t + d\\tilde{J}_t$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Microstructure Noise:\n",
    "- Quote Spread Noise: The price fluctuation caused by trades alternating between bid and ask prices, creating a \"bouncing\" effect that obscures the true efficient price.\n",
    "\n",
    "- Order Flow Noise: Price movements driven by the imbalance between buy and sell orders, where persistent directional trading pressure can create temporary price trends away from the efficient price.\n",
    "\n",
    "- Strategic Order Noise: Price distortions created when large traders split their orders into smaller pieces to minimize market impact.\n",
    "\n",
    "- Quote Positioning Noise: Price effects from market makers strategically placing and canceling quotes to create false impressions of supply and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating efficient price process...\n",
      "Adding quote spread noise...\n",
      "Simulating order flow noise...\n",
      "Simulating strategic order splitting...\n",
      "Simulating strategic quote positioning...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03378606,  0.03567945,  0.09798677,  0.10495182, -0.12444308,\n",
       "       -0.01157954, -0.20460404,  0.04836754, -0.03279175,  0.04115321])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 1000 noise samples\n",
    "from microstructure_simulator import MarketMicrostructureSimulator\n",
    "\n",
    "simulator = MarketMicrostructureSimulator(n_samples=1000)\n",
    "result = simulator.simulate_full_microstructure()\n",
    "noise_components = simulator.extract_noise_components(result)\n",
    "noise_components[\"total_noise\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n",
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume  Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from preprocess_td import preprocess_tick_data\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')\n",
    "\n",
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aleksandr/Desktop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/my_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[:\u001b[38;5;241m2000\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save DataFrame to CSV\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/aleksandr/Desktop\" + \"/my_data.csv\"\n",
    "df_sample = df[:2000]\n",
    "# Save DataFrame to CSV\n",
    "df_sample.to_csv(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/my_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                          Timestamp   Value  Volume  Volatility\n",
       "0  2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1  2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2  2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3  2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4  2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:200]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting TDA volatility regime pipeline at 20:04:00\n",
      "\n",
      "--- STEP 1: Extracting Microstructure Features ---\n",
      "Time features computed in 0.01 seconds\n",
      "Computing microstructure features...\n",
      "Microstructure features computed in 0.01 seconds\n",
      "Microstructure features computed in 0.01 seconds\n",
      "Computing order flow metrics...\n",
      "Order flow metrics computed in 0.00 seconds\n",
      "Order flow metrics computed in 0.00 seconds\n",
      "Computing DMI features...\n",
      "DMI features computed in 0.01 seconds\n",
      "DMI features computed in 0.01 seconds\n",
      "Extracted 46 features in total 0.03 seconds\n",
      "Extracted 46 microstructure features\n",
      "Feature extraction completed in 0.03 seconds\n",
      "\n",
      "--- STEP 2: Enhancing Features with Information Theory ---\n",
      "Estimating Shannon entropy with heavy-tail adjustment...\n",
      "Shannon entropy estimation completed in 0.05 seconds\n",
      "Computing mutual information matrix using KDE...\n",
      "Mutual information computation completed in 3.49 seconds\n",
      "\n",
      "======================================================================\n",
      "COMPUTING TRANSFER ENTROPY: Analyzing causal information flow between variables\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Transfer entropy computation completed in 0.52 seconds\n",
      "Maximum transfer entropy value: 0.3288\n",
      "======================================================================\n",
      "\n",
      "Creating enhanced feature set...\n",
      "Ranking features by importance...\n",
      "Feature ranking completed in 0.00 seconds\n",
      "Computing KL divergence between current and recent windows...\n",
      "KL divergence computation completed in 0.71 seconds\n",
      "Feature enhancement completed in 0.71 seconds\n",
      "Enhanced feature set contains 40 features\n",
      "Enhanced features: 40 features selected\n",
      "Feature enhancement completed in 4.77 seconds\n",
      "\n",
      "--- STEP 3: Performing Topological Data Analysis ---\n",
      "Transformed MI matrix to match enhanced features: (40, 40)\n",
      "Transformed TE matrix to match enhanced features: (40, 40)\n",
      "Computing temporally-weighted distance matrix...\n",
      "Applied MI-based feature weighting: min=1.0000, max=1.0000\n",
      "Applied TE-based feature weighting: min=1.0760, max=2.0000\n",
      "Distance matrix computation completed in 0.01 seconds\n",
      "Distance matrix shape: (200, 200)\n",
      "Distance range: [0.0000, 16942.0248]\n",
      "Computing persistent path zigzag homology for market microstructure...\n",
      "Network creation completed in 0.00 seconds\n",
      "Computing path complexes for 3 windows...\n",
      "Computing path complex for window 1/3...\n",
      "Computing path complex with max path length 3...\n",
      "Building dimension 2 paths...\n",
      "Created 4 paths in dimension 2 (0.00s)\n",
      "Building dimension 3 paths...\n",
      "Created 1 paths in dimension 3 (0.00s)\n",
      "Window 1 path complex sizes: dim 0: 100, dim 1: 6, dim 2: 4, dim 3: 1\n",
      "Computing path complex for window 2/3...\n",
      "Computing path complex with max path length 3...\n",
      "Building dimension 2 paths...\n",
      "Created 0 paths in dimension 2 (0.00s)\n",
      "Building dimension 3 paths...\n",
      "Created 0 paths in dimension 3 (0.00s)\n",
      "Window 2 path complex sizes: dim 0: 100, dim 1: 0, dim 2: 0, dim 3: 0\n",
      "Computing path complex for window 3/3...\n",
      "Computing path complex with max path length 3...\n",
      "Building dimension 2 paths...\n",
      "Created 0 paths in dimension 2 (0.00s)\n",
      "Building dimension 3 paths...\n",
      "Created 0 paths in dimension 3 (0.00s)\n",
      "Window 3 path complex sizes: dim 0: 100, dim 1: 0, dim 2: 0, dim 3: 0\n",
      "Processing homology in dimension 0...\n",
      "Dimension 0 processing completed in 0.00 seconds\n",
      "Processing homology in dimension 1...\n",
      "Dimension 1 processing completed in 0.00 seconds\n",
      "Processing homology in dimension 2...\n",
      "Dimension 2 processing completed in 0.00 seconds\n",
      "Processing homology in dimension 3...\n",
      "Dimension 3 processing completed in 0.00 seconds\n",
      "Computing transitions between windows...\n",
      "Total path zigzag persistence computation completed in 0.00 seconds\n",
      "Identifying 4 volatility regimes...\n",
      "Extracting topological features...\n",
      "No persistence computations found. Computing persistent homology...\n",
      "Computing persistent homology (max dim=2)...\n",
      "Building directed filtration (max dim=2)...\n",
      "Building sparse directed network...\n",
      "Built sparse directed network with 10 edges in 0.00 seconds\n",
      "Sparsity: 0.000250\n",
      "Filtration built with 220 simplices in 0.00 seconds\n",
      "Simplex counts by dimension:\n",
      "  Dimension 0: 200\n",
      "  Dimension 1: 210\n",
      "  Dimension 2: 220\n",
      "Computed persistent homology in 0.01 seconds\n",
      "Persistence diagram summary:\n",
      "  Dimension 0: 200 persistence pairs\n",
      "  Dimension 1: 0 persistence pairs\n",
      "  Dimension 2: 0 persistence pairs\n",
      "Extracted 24 topological features in 0.01 seconds\n",
      "Combined feature matrix shape: (200, 64)\n",
      "Affinity matrix range: [0.0000, 1.0000]\n",
      "Final regime distribution: [61 94 17 28]\n",
      "Regime balance ratio: 0.181\n",
      "Regime identification completed in 0.18 seconds\n",
      "Identified 4 volatility regimes\n",
      "Topological analysis completed in 0.19 seconds\n",
      "\n",
      "Total execution time: 5.00 seconds (0.08 minutes)\n",
      "Model saved to volatility_model.pkl\n"
     ]
    }
   ],
   "source": [
    "from volatility_regimes_identification.tda_volatility_pipeline import TDAVolatilityPipeline\n",
    "\n",
    "# Initialize the pipeline\n",
    "pipeline = TDAVolatilityPipeline()\n",
    "\n",
    "# Process data through the pipeline\n",
    "labeled_df = pipeline.process_data(\n",
    "    df, \n",
    "    timestamp_col='Timestamp',\n",
    "    price_col='Value',\n",
    "    volume_col='Volume',\n",
    "    volatility_col='Volatility',\n",
    "    n_regimes=4\n",
    ")\n",
    "\n",
    "# Save the trained model\n",
    "pipeline.save_model('volatility_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential regime counts:\n",
      "\n",
      "Regime 1:\n",
      "Number of sequences: 3\n",
      "Sequence lengths: [57, 2, 35]\n",
      "Average sequence length: 31.33\n",
      "\n",
      "Regime 0:\n",
      "Number of sequences: 2\n",
      "Sequence lengths: [1, 60]\n",
      "Average sequence length: 30.50\n",
      "\n",
      "Regime 2:\n",
      "Number of sequences: 1\n",
      "Sequence lengths: [17]\n",
      "Average sequence length: 17.00\n",
      "\n",
      "Regime 3:\n",
      "Number of sequences: 1\n",
      "Sequence lengths: [28]\n",
      "Average sequence length: 28.00\n"
     ]
    }
   ],
   "source": [
    "# Get the regime values as a list\n",
    "regimes = labeled_df[\"regime\"].tolist()\n",
    "\n",
    "# Initialize counters\n",
    "current_regime = regimes[0]\n",
    "current_count = 1\n",
    "regime_sequences = {}\n",
    "\n",
    "# Count sequential regimes\n",
    "for i in range(1, len(regimes)):\n",
    "    if regimes[i] == current_regime:\n",
    "        current_count += 1\n",
    "    else:\n",
    "        # Store the sequence count\n",
    "        key = f\"Regime {current_regime}\"\n",
    "        if key not in regime_sequences:\n",
    "            regime_sequences[key] = []\n",
    "        regime_sequences[key].append(current_count)\n",
    "        \n",
    "        # Reset counters\n",
    "        current_regime = regimes[i]\n",
    "        current_count = 1\n",
    "\n",
    "# Add the last sequence\n",
    "key = f\"Regime {current_regime}\"\n",
    "if key not in regime_sequences:\n",
    "    regime_sequences[key] = []\n",
    "regime_sequences[key].append(current_count)\n",
    "\n",
    "# Print results\n",
    "print(\"Sequential regime counts:\")\n",
    "for regime, sequences in regime_sequences.items():\n",
    "    print(f\"\\n{regime}:\")\n",
    "    print(f\"Number of sequences: {len(sequences)}\")\n",
    "    print(f\"Sequence lengths: {sequences}\")\n",
    "    print(f\"Average sequence length: {sum(sequences)/len(sequences):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Adaptations\n",
    "\n",
    "Modify the original CSDI code (GitHub) for denoising:\n",
    "\n",
    "### Remove Imputation Logic:\n",
    "- Delete code blocks that handle missing value imputation.\n",
    "\n",
    "### Time Embeddings:\n",
    "- Encode irregular timestamps as sinusoidal embeddings (normalized to [0,1]).\n",
    "\n",
    "### Conditioning Mechanism:\n",
    "- Use the noisy input as the conditional context (instead of partial observations).\n",
    "\n",
    "### Diffusion Process:\n",
    "- Use the original diffusion steps but disable masking (all positions are observed).\n",
    "- Adjust the noise schedule (β) to match financial noise characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training Pipeline\n",
    "\n",
    "**Objective:** Learn to reverse the diffusion process conditioned on noisy ticks.\n",
    "\n",
    "### Inputs:\n",
    "- **noisy_data:** Corrupted ticks (price, volume, etc.).\n",
    "- **mask:** All ones (no missing data).\n",
    "- **time_embeddings:** Encoded timestamps.\n",
    "\n",
    "### Forward Process:\n",
    "- Gradually add Gaussian noise to `noisy_data` across diffusion timesteps.\n",
    "- Forward Process: Replace Gaussian SDE with a market-realistic stochastic process.\n",
    "\n",
    "### Reverse Process:\n",
    "- Train the model to predict the score (gradient) to denoise the data.\n",
    "\n",
    "### Loss Function:\n",
    "- Weighted MSE between predicted and true noise at each diffusion step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Functions to Implement\n",
    "\n",
    "### Data Loader:\n",
    "```python\n",
    "def load_tick_data():\n",
    "    \"\"\"Reads raw ticks and converts to windowed sequences.\"\"\"\n",
    "    pass\n",
    "\n",
    "def inject_microstructure_noise():\n",
    "    \"\"\"Adds synthetic bid-ask bounce, order flow noise.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Time Embeddings:\n",
    "```python\n",
    "def encode_timestamps():\n",
    "    \"\"\"Converts irregular timestamps to continuous embeddings.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Diffusion Utils:\n",
    "```python\n",
    "def beta_scheduler():\n",
    "    \"\"\"Defines the noise schedule (linear, cosine, etc.).\"\"\"\n",
    "    pass\n",
    "\n",
    "def q_sample():\n",
    "    \"\"\"Forward diffusion process (adding noise).\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Model:\n",
    "```python\n",
    "class ConditionalScoreModel:\n",
    "    \"\"\"Modified CSDI backbone (transformer/TCN).\"\"\"\n",
    "    pass\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"Computes loss and updates weights.\"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Process\n",
    "\n",
    "### Hyperparameters:\n",
    "- Diffusion steps (`T=1000`), learning rate (`1e-4`), batch size (`64`).\n",
    "- Noise schedule (e.g., `beta_start=1e-4`, `beta_end=0.02`).\n",
    "\n",
    "### Training Loop:\n",
    "For each batch:\n",
    "1. Generate noisy data via `inject_microstructure_noise()`.\n",
    "2. Compute time embeddings for irregular timestamps.\n",
    "3. **Forward pass:** Corrupt noisy data with diffusion.\n",
    "4. **Reverse pass:** Predict denoised data.\n",
    "5. Update model weights via gradient descent.\n",
    "\n",
    "### Checkpointing:\n",
    "- Save model weights periodically (e.g., every epoch).\n",
    "- Track validation loss on a held-out tick dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Testing\n",
    "\n",
    "### Metrics:\n",
    "- **Reconstruction Loss:** MSE between denoised and clean data (if synthetic).\n",
    "- **Volatility Consistency:** Compare realized volatility of raw vs. denoised data.\n",
    "- **Microstructure Preservation:** Autocorrelation of trade signs.\n",
    "\n",
    "### Visual Checks:\n",
    "- Plot raw vs. denoised ticks (ensure no new timestamps are added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Pipeline\n",
    "\n",
    "### Preprocessing:\n",
    "- Normalize new tick data using the same scaler from training.\n",
    "- Encode timestamps.\n",
    "\n",
    "### Inference:\n",
    "- Run the trained CSDI in reverse diffusion mode with `mask=1`.\n",
    "\n",
    "### Postprocessing:\n",
    "- Inverse-transform normalized denoised data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
