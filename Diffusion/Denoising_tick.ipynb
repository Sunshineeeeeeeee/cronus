{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Preprocessing\n",
    "\n",
    "### Cleaning + Estimating volatility\n",
    "- Ensuring validity of datapoints \n",
    "- Cleaning out deviations | Isolated points | etc ...\n",
    "- Estimating Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n"
     ]
    }
   ],
   "source": [
    "from preprocess_td import preprocess_tick_data\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    }
   ],
   "source": [
    "# Would be lovely to estimate parameters of function\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['return', 'SYMBOL'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mreturn\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mSYMBOL\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwavelet_vol\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolatility\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m                   \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTIMESTAMP\u001b[39m\u001b[38;5;124m'\u001b[39m:\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTimestamp\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      4\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVALUE\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValue\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m      5\u001b[0m                    \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVOLUME\u001b[39m\u001b[38;5;124m'\u001b[39m : \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVolume\u001b[39m\u001b[38;5;124m'\u001b[39m}, inplace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      6\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/frame.py:5581\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   5433\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   5434\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   5435\u001b[0m     labels: IndexLabel \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5442\u001b[0m     errors: IgnoreRaise \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   5443\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   5444\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   5445\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   5446\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   5579\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   5580\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 5581\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   5582\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5583\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5584\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5585\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5586\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5587\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5588\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   5589\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:4788\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4786\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4787\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4788\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4790\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4791\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/generic.py:4830\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, only_slice)\u001b[0m\n\u001b[1;32m   4828\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4829\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4830\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4831\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4833\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4834\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:7070\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   7068\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   7069\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 7070\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask]\u001b[38;5;241m.\u001b[39mtolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   7071\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   7072\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['return', 'SYMBOL'] not found in axis\""
     ]
    }
   ],
   "source": [
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Noise Injection \n",
    "\n",
    "Suppose that, the latent log-price Xt is an Ito-semimartingale of the form \n",
    "\n",
    "$dX_t = b_t dt + \\sigma_t dW_t + dJ_t,$  \n",
    "$d\\sigma_t = \\tilde{b}_t dt + \\tilde{\\sigma}^{(1)}_t dW_t + \\tilde{\\sigma}^{(2)}_t d\\tilde{W}_t + d\\tilde{J}_t$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Microstructure Noise:\n",
    "- Quote Spread Noise: The price fluctuation caused by trades alternating between bid and ask prices, creating a \"bouncing\" effect that obscures the true efficient price.\n",
    "\n",
    "- Order Flow Noise: Price movements driven by the imbalance between buy and sell orders, where persistent directional trading pressure can create temporary price trends away from the efficient price.\n",
    "\n",
    "- Strategic Order Noise: Price distortions created when large traders split their orders into smaller pieces to minimize market impact.\n",
    "\n",
    "- Quote Positioning Noise: Price effects from market makers strategically placing and canceling quotes to create false impressions of supply and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating efficient price process...\n",
      "Adding quote spread noise...\n",
      "Simulating order flow noise...\n",
      "Simulating strategic order splitting...\n",
      "Simulating strategic quote positioning...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03378606,  0.03567945,  0.09798677,  0.10495182, -0.12444308,\n",
       "       -0.01157954, -0.20460404,  0.04836754, -0.03279175,  0.04115321])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 1000 noise samples\n",
    "from microstructure_simulator import MarketMicrostructureSimulator\n",
    "\n",
    "simulator = MarketMicrostructureSimulator(n_samples=1000)\n",
    "result = simulator.simulate_full_microstructure()\n",
    "noise_components = simulator.extract_noise_components(result)\n",
    "noise_components[\"total_noise\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n",
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume  Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from preprocess_td import preprocess_tick_data\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')\n",
    "\n",
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aleksandr/Desktop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/my_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[:\u001b[38;5;241m2000\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save DataFrame to CSV\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/aleksandr/Desktop\" + \"/my_data.csv\"\n",
    "df_sample = df[:2000]\n",
    "# Save DataFrame to CSV\n",
    "df_sample.to_csv(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/my_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Beginning volatility regime detection...\n",
      "\n",
      "Identifying volatility regimes...\n",
      "Beginning two-stage volatility regime detection...\n",
      "\n",
      "=== Stage 1: Learning Patterns from Sample ===\n",
      "\n",
      "--- STEP 1: Computing Features ---\n",
      "\n",
      "--- STEP 1: Computing Microstructure Features ---\n",
      "Computing microstructure features...\n",
      "Computing order flow metrics...\n",
      "Computing DMI features...\n",
      "Extracted 46 features\n",
      "\n",
      "--- STEP 2: Enhancing Features ---\n",
      "\n",
      "--- STEP 2: Applying Information-Theoretic Feature Enhancement ---\n",
      "Estimating Shannon entropy with heavy-tail adjustment...\n",
      "Computing mutual information matrix using KDE...\n",
      "Computing KL divergence between current and recent windows...\n",
      "Ranking features by importance...\n",
      "Creating enhanced feature set...\n",
      "\n",
      "--- STEP 3: Detecting Regimes ---\n",
      "\n",
      "=== Detecting Volatility Regimes ===\n",
      "\n",
      "--- STEP 1: Initializing TDA Pipeline ---\n",
      "\n",
      "--- STEP 2: Computing Temporal Distance Matrix ---\n",
      "Computing temporally-weighted distance matrix...\n",
      "\n",
      "--- STEP 3: Computing Persistent Homology ---\n",
      "Computing persistent homology using GUDHI...\n",
      "Using standard Rips complex...\n",
      "Creating simplex tree...\n",
      "Simplex tree has 642975 simplices\n",
      "Dimension is 2\n",
      "Computing persistence...\n",
      "Found 206 persistence pairs\n",
      "Generating visualizations...\n",
      "\n",
      "--- STEP 4: Creating Mapper Graph ---\n",
      "Creating temporal mapper graph...\n",
      "\n",
      "--- STEP 5: Identifying Regimes ---\n",
      "Identifying volatility regimes...\n",
      "\n",
      "Regime detection completed.\n",
      "Analyzing volatility regimes...\n",
      "\n",
      "=== Stage 2: Applying Patterns to Full Dataset ===\n",
      "Applying learned patterns to full dataset...\n",
      "Computing microstructure features...\n",
      "\n",
      "--- STEP 1: Computing Microstructure Features ---\n",
      "Computing microstructure features...\n",
      "Computing order flow metrics...\n",
      "Computing DMI features...\n",
      "Extracted 46 features\n",
      "\n",
      "--- STEP 2: Applying Information-Theoretic Feature Enhancement ---\n",
      "Estimating Shannon entropy with heavy-tail adjustment...\n",
      "Computing mutual information matrix using KDE...\n",
      "Computing KL divergence between current and recent windows...\n",
      "Ranking features by importance...\n",
      "Creating enhanced feature set...\n",
      "Labeling new data with existing regimes...\n",
      "Computing microstructure features...\n",
      "Computing order flow metrics...\n",
      "Computing DMI features...\n",
      "Extracted 46 features\n",
      "Labeled 250 new data points\n",
      "Saved data with regime labels to /Users/aleksandr/code/scripts/Diffusion/volatility_regimes_identification/results/tick_data_with_regimes.csv\n",
      "Model saved to /Users/aleksandr/code/scripts/Diffusion/volatility_regimes_identification/results/regime_model.pkl\n",
      "\n",
      "Saved model to /Users/aleksandr/code/scripts/Diffusion/volatility_regimes_identification/results/regime_model.pkl\n",
      "\n",
      "Regime detection completed in 0:00:14.135526\n",
      "\n",
      "=== Volatility Regime Statistics ===\n",
      "Regime 1:\n",
      "  - Size: 204 ticks (81.6% of data)\n",
      "  - Mean volatility: 0.000279\n",
      "  - Duration: nan seconds\n",
      "\n",
      "Regime 2:\n",
      "  - Size: 21 ticks (8.4% of data)\n",
      "  - Mean volatility: 0.000263\n",
      "  - Duration: nan seconds\n",
      "\n",
      "Regime 3:\n",
      "  - Size: 25 ticks (10.0% of data)\n",
      "  - Mean volatility: 0.000269\n",
      "  - Duration: nan seconds\n",
      "\n",
      "\n",
      "=== Regime Transition Probabilities ===\n",
      "  Regime 1 → Regime 1: 0.990\n",
      "  Regime 1 → Regime 3: 0.010\n",
      "  Regime 2 → Regime 2: 0.810\n",
      "  Regime 2 → Regime 3: 0.190\n",
      "  Regime 3 → Regime 1: 0.120\n",
      "  Regime 3 → Regime 2: 0.120\n",
      "  Regime 3 → Regime 3: 0.760\n"
     ]
    }
   ],
   "source": [
    "# Import the main identifier class\n",
    "from volatility_regimes_identifier import VolatilityRegimesIdentifier\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample the data\n",
    "df_sample = df[:250]\n",
    "\n",
    "print(\"Beginning volatility regime detection...\")\n",
    "start_time = datetime.now()\n",
    "\n",
    "# Initialize the identifier with our data\n",
    "identifier = VolatilityRegimesIdentifier()\n",
    "\n",
    "# Identify volatility regimes\n",
    "print(\"\\nIdentifying volatility regimes...\")\n",
    "df_with_regimes = identifier.identify_regimes(\n",
    "    df_sample,\n",
    "    timestamp_col='Timestamp',\n",
    "    price_col='Value',\n",
    "    volume_col='Volume',\n",
    "    volatility_col='Volatility',\n",
    "    n_regimes=3,  # Set to None for automatic regime detection\n",
    "    window_sizes=[10, 30, 50],  # Window sizes for feature extraction\n",
    "    top_features=10,  # Number of top features to use\n",
    "    alpha=0.5,  # Weight for temporal component\n",
    "    beta=0.1,  # Decay rate for temporal distance\n",
    "    sample_size=200,  # Size of sample to use (matching our df_sample size)\n",
    "    sampling_method='sequential'  # Method to use for sampling\n",
    ")\n",
    "\n",
    "end_time = datetime.now()\n",
    "print(f\"\\nRegime detection completed in {end_time - start_time}\")\n",
    "\n",
    "# Get regime statistics\n",
    "regime_stats = identifier.get_regime_statistics()\n",
    "\n",
    "# Print regime statistics\n",
    "print(\"\\n=== Volatility Regime Statistics ===\")\n",
    "for stat in regime_stats['regime_stats']:\n",
    "    regime_id = stat['regime_id']\n",
    "    size = stat['size']\n",
    "    mean_vol = stat.get('mean_vol', float('nan'))\n",
    "    duration = stat.get('duration', float('nan'))\n",
    "    \n",
    "    print(f\"Regime {regime_id+1}:\")\n",
    "    print(f\"  - Size: {size} ticks ({size/len(df_sample)*100:.1f}% of data)\")\n",
    "    print(f\"  - Mean volatility: {mean_vol:.6f}\")\n",
    "    print(f\"  - Duration: {duration:.2f} seconds\")\n",
    "    print()\n",
    "\n",
    "# Print transition probabilities\n",
    "print(\"\\n=== Regime Transition Probabilities ===\")\n",
    "transition_probs = regime_stats['transition_probs']\n",
    "for i in range(len(transition_probs)):\n",
    "    for j in range(len(transition_probs[i])):\n",
    "        if transition_probs[i, j] > 0:\n",
    "            print(f\"  Regime {i+1} → Regime {j+1}: {transition_probs[i, j]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Windowing data\n",
    "- Split the series into overlapping or non-overlapping windows.\n",
    "- Ensure no future leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Adaptations\n",
    "\n",
    "Modify the original CSDI code (GitHub) for denoising:\n",
    "\n",
    "### Remove Imputation Logic:\n",
    "- Delete code blocks that handle missing value imputation.\n",
    "\n",
    "### Time Embeddings:\n",
    "- Encode irregular timestamps as sinusoidal embeddings (normalized to [0,1]).\n",
    "\n",
    "### Conditioning Mechanism:\n",
    "- Use the noisy input as the conditional context (instead of partial observations).\n",
    "\n",
    "### Diffusion Process:\n",
    "- Use the original diffusion steps but disable masking (all positions are observed).\n",
    "- Adjust the noise schedule (β) to match financial noise characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training Pipeline\n",
    "\n",
    "**Objective:** Learn to reverse the diffusion process conditioned on noisy ticks.\n",
    "\n",
    "### Inputs:\n",
    "- **noisy_data:** Corrupted ticks (price, volume, etc.).\n",
    "- **mask:** All ones (no missing data).\n",
    "- **time_embeddings:** Encoded timestamps.\n",
    "\n",
    "### Forward Process:\n",
    "- Gradually add Gaussian noise to `noisy_data` across diffusion timesteps.\n",
    "- Forward Process: Replace Gaussian SDE with a market-realistic stochastic process.\n",
    "\n",
    "### Reverse Process:\n",
    "- Train the model to predict the score (gradient) to denoise the data.\n",
    "\n",
    "### Loss Function:\n",
    "- Weighted MSE between predicted and true noise at each diffusion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Functions to Implement\n",
    "\n",
    "### Data Loader:\n",
    "```python\n",
    "def load_tick_data():\n",
    "    \"\"\"Reads raw ticks and converts to windowed sequences.\"\"\"\n",
    "    pass\n",
    "\n",
    "def inject_microstructure_noise():\n",
    "    \"\"\"Adds synthetic bid-ask bounce, order flow noise.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Time Embeddings:\n",
    "```python\n",
    "def encode_timestamps():\n",
    "    \"\"\"Converts irregular timestamps to continuous embeddings.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Diffusion Utils:\n",
    "```python\n",
    "def beta_scheduler():\n",
    "    \"\"\"Defines the noise schedule (linear, cosine, etc.).\"\"\"\n",
    "    pass\n",
    "\n",
    "def q_sample():\n",
    "    \"\"\"Forward diffusion process (adding noise).\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Model:\n",
    "```python\n",
    "class ConditionalScoreModel:\n",
    "    \"\"\"Modified CSDI backbone (transformer/TCN).\"\"\"\n",
    "    pass\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"Computes loss and updates weights.\"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Process\n",
    "\n",
    "### Hyperparameters:\n",
    "- Diffusion steps (`T=1000`), learning rate (`1e-4`), batch size (`64`).\n",
    "- Noise schedule (e.g., `beta_start=1e-4`, `beta_end=0.02`).\n",
    "\n",
    "### Training Loop:\n",
    "For each batch:\n",
    "1. Generate noisy data via `inject_microstructure_noise()`.\n",
    "2. Compute time embeddings for irregular timestamps.\n",
    "3. **Forward pass:** Corrupt noisy data with diffusion.\n",
    "4. **Reverse pass:** Predict denoised data.\n",
    "5. Update model weights via gradient descent.\n",
    "\n",
    "### Checkpointing:\n",
    "- Save model weights periodically (e.g., every epoch).\n",
    "- Track validation loss on a held-out tick dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Testing\n",
    "\n",
    "### Metrics:\n",
    "- **Reconstruction Loss:** MSE between denoised and clean data (if synthetic).\n",
    "- **Volatility Consistency:** Compare realized volatility of raw vs. denoised data.\n",
    "- **Microstructure Preservation:** Autocorrelation of trade signs.\n",
    "\n",
    "### Visual Checks:\n",
    "- Plot raw vs. denoised ticks (ensure no new timestamps are added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Pipeline\n",
    "\n",
    "### Preprocessing:\n",
    "- Normalize new tick data using the same scaler from training.\n",
    "- Encode timestamps.\n",
    "\n",
    "### Inference:\n",
    "- Run the trained CSDI in reverse diffusion mode with `mask=1`.\n",
    "\n",
    "### Postprocessing:\n",
    "- Inverse-transform normalized denoised data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
