{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Preprocessing\n",
    "\n",
    "### Cleaning + Estimating volatility\n",
    "- Ensuring validity of datapoints \n",
    "- Cleaning out deviations | Isolated points | etc ...\n",
    "- Estimating Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n"
     ]
    }
   ],
   "source": [
    "from preprocess_td import preprocess_tick_data\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    }
   ],
   "source": [
    "# Would be lovely to estimate parameters of function\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume  Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Noise Injection \n",
    "\n",
    "Suppose that, the latent log-price Xt is an Ito-semimartingale of the form \n",
    "\n",
    "$dX_t = b_t dt + \\sigma_t dW_t + dJ_t,$  \n",
    "$d\\sigma_t = \\tilde{b}_t dt + \\tilde{\\sigma}^{(1)}_t dW_t + \\tilde{\\sigma}^{(2)}_t d\\tilde{W}_t + d\\tilde{J}_t$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Microstructure Noise:\n",
    "- Quote Spread Noise: The price fluctuation caused by trades alternating between bid and ask prices, creating a \"bouncing\" effect that obscures the true efficient price.\n",
    "\n",
    "- Order Flow Noise: Price movements driven by the imbalance between buy and sell orders, where persistent directional trading pressure can create temporary price trends away from the efficient price.\n",
    "\n",
    "- Strategic Order Noise: Price distortions created when large traders split their orders into smaller pieces to minimize market impact.\n",
    "\n",
    "- Quote Positioning Noise: Price effects from market makers strategically placing and canceling quotes to create false impressions of supply and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating efficient price process...\n",
      "Adding quote spread noise...\n",
      "Simulating order flow noise...\n",
      "Simulating strategic order splitting...\n",
      "Simulating strategic quote positioning...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.01059243, -0.02092213, -0.0834803 , -0.00851905, -0.01510907,\n",
       "       -0.12214871, -0.08950043,  0.02924293, -0.08329231, -0.03416489])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 1000 noise samples\n",
    "from microstructure_simulator import MarketMicrostructureSimulator\n",
    "\n",
    "simulator = MarketMicrostructureSimulator(n_samples=1000)\n",
    "result = simulator.simulate_full_microstructure()\n",
    "noise_components = simulator.extract_noise_components(result)\n",
    "noise_components[\"total_noise\"][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Windowing data\n",
    "- Split the series into overlapping or non-overlapping windows.\n",
    "- Ensure no future leakage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VolatilityWindowGenerator:\n",
    "    \"\"\"\n",
    "    Generates windows based on volatility regimes and provides tensor conversion utilities.\n",
    "    \n",
    "    This class handles:\n",
    "    1. Volatility regime detection and window creation\n",
    "    2. Conversion to tensor representations\n",
    "    3. Train/val/test splitting\n",
    "    4. Window analysis and diagnostics\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, feature_columns=['Value', 'Volume', 'Volatility']):\n",
    "        \"\"\"\n",
    "        Initialize the window generator with tick data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        df : pandas.DataFrame\n",
    "            DataFrame with columns ['Timestamp', 'Value', 'Volume', 'Volatility']\n",
    "        feature_columns : list\n",
    "            Columns to include as features\n",
    "        \"\"\"\n",
    "        self.df = df.copy()\n",
    "        self.feature_columns = feature_columns\n",
    "        self.windows = []\n",
    "        self.window_tensors = None\n",
    "        self.window_info = None\n",
    "    \n",
    "    def detect_volatility_regimes(self, sensitivity=1.5, min_regime_size=30, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Detect regime changes based on significant volatility shifts.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        sensitivity : float\n",
    "            Multiplier for standard deviation to detect regime changes\n",
    "        min_regime_size : int\n",
    "            Minimum number of ticks in a regime\n",
    "        smoothing_window : int\n",
    "            Window size for volatility smoothing\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        list of int\n",
    "            Indices where regime changes occur\n",
    "        \"\"\"\n",
    "        # Smooth volatility for cleaner regime detection\n",
    "        smoothed_vol = self.df['Volatility'].rolling(window=smoothing_window, min_periods=1).mean()\n",
    "        \n",
    "        # Compute volatility changes\n",
    "        vol_changes = smoothed_vol.diff().abs()\n",
    "        \n",
    "        # Identify significant changes (> sensitivity * std)\n",
    "        std_change = vol_changes.std()\n",
    "        threshold = sensitivity * std_change\n",
    "        \n",
    "        # Get indices of significant changes\n",
    "        potential_breakpoints = list(vol_changes[vol_changes > threshold].index)\n",
    "        \n",
    "        # Filter breakpoints to ensure minimum regime size\n",
    "        breakpoints = [0]  # Always start with the first point\n",
    "        for bp in potential_breakpoints:\n",
    "            if bp - breakpoints[-1] >= min_regime_size:\n",
    "                breakpoints.append(bp)\n",
    "        \n",
    "        # Add the end of series if the last segment is long enough\n",
    "        if len(self.df) - breakpoints[-1] >= min_regime_size:\n",
    "            breakpoints.append(len(self.df))\n",
    "        else:\n",
    "            # Adjust the last breakpoint to include all data\n",
    "            breakpoints[-1] = len(self.df)\n",
    "        \n",
    "        print(f\"Detected {len(breakpoints)-1} volatility regimes\")\n",
    "        return breakpoints\n",
    "    \n",
    "    def create_regime_windows(self, sensitivity=1.5, min_regime_size=30, \n",
    "                            max_window_size=200, smoothing_window=5):\n",
    "        \"\"\"\n",
    "        Create windows based on detected volatility regimes.\n",
    "        \"\"\"\n",
    "        # Get regime breakpoints\n",
    "        breakpoints = self.detect_volatility_regimes(\n",
    "            sensitivity=sensitivity, \n",
    "            min_regime_size=min_regime_size,\n",
    "            smoothing_window=smoothing_window\n",
    "        )\n",
    "        \n",
    "        windows = []\n",
    "        window_id = 0\n",
    "        \n",
    "        # Create windows from regimes\n",
    "        for i in range(len(breakpoints) - 1):\n",
    "            start_idx = breakpoints[i]\n",
    "            end_idx = breakpoints[i+1]\n",
    "            \n",
    "            regime_length = end_idx - start_idx\n",
    "            \n",
    "            # Skip if regime is too small\n",
    "            if regime_length < min_regime_size:\n",
    "                continue\n",
    "                \n",
    "            # If regime is too large, split it into multiple windows\n",
    "            if regime_length > max_window_size:\n",
    "                # Calculate number of windows needed\n",
    "                n_windows = int(np.ceil(regime_length / max_window_size))\n",
    "                window_size = regime_length // n_windows  # Ensure even division\n",
    "                \n",
    "                # Create windows of equal size\n",
    "                for j in range(n_windows):\n",
    "                    seg_start = start_idx + (j * window_size)\n",
    "                    seg_end = start_idx + ((j + 1) * window_size)\n",
    "                    \n",
    "                    # Adjust last window to include any remaining points\n",
    "                    if j == n_windows - 1:\n",
    "                        seg_end = end_idx\n",
    "                    \n",
    "                    window = self.df.iloc[seg_start:seg_end].copy()\n",
    "                    \n",
    "                    # Only add if window has data\n",
    "                    if len(window) >= min_regime_size:\n",
    "                        window['window_start'] = seg_start\n",
    "                        window['window_end'] = seg_end\n",
    "                        window['window_id'] = window_id\n",
    "                        window['regime_id'] = i\n",
    "                        windows.append(window)\n",
    "                        window_id += 1\n",
    "            else:\n",
    "                # Use the entire regime as a window\n",
    "                window = self.df.iloc[start_idx:end_idx].copy()\n",
    "                if len(window) >= min_regime_size:\n",
    "                    window['window_start'] = start_idx\n",
    "                    window['window_end'] = end_idx\n",
    "                    window['window_id'] = window_id\n",
    "                    window['regime_id'] = i\n",
    "                    windows.append(window)\n",
    "                    window_id += 1\n",
    "        \n",
    "        self.windows = windows\n",
    "        \n",
    "        print(f\"Created {len(windows)} windows from {len(breakpoints)-1} regimes\")\n",
    "        if windows:  # Only print statistics if there are windows\n",
    "            print(f\"Average window size: {sum(len(w) for w in windows) / len(windows):.2f}\")\n",
    "            print(f\"Min window size: {min(len(w) for w in windows)}\")\n",
    "            print(f\"Max window size: {max(len(w) for w in windows)}\")\n",
    "        else:\n",
    "            print(\"Warning: No windows were created!\")\n",
    "        \n",
    "        return windows\n",
    "    \n",
    "    def create_adaptive_windows(self, base_window_size=100, min_window_size=50, \n",
    "                              max_window_size=200, overlap_ratio=0.0):\n",
    "        \"\"\"\n",
    "        Create windows with adaptive sizing based on volatility levels.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        base_window_size : int\n",
    "            Base window size for median volatility\n",
    "        min_window_size : int\n",
    "            Minimum window size for high volatility periods\n",
    "        max_window_size : int\n",
    "            Maximum window size for low volatility periods\n",
    "        overlap_ratio : float\n",
    "            Ratio of overlap between consecutive windows (0 to 1)\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        list of DataFrames\n",
    "            List of windowed DataFrames\n",
    "        \"\"\"\n",
    "        # Calculate median volatility for scaling\n",
    "        median_vol = self.df['Volatility'].median()\n",
    "        \n",
    "        windows = []\n",
    "        i = 0\n",
    "        window_id = 0\n",
    "        \n",
    "        while i < len(self.df):\n",
    "            # Get current volatility\n",
    "            current_vol = self.df['Volatility'].iloc[i]\n",
    "            \n",
    "            # Adjust window size based on volatility\n",
    "            # Higher volatility = smaller window, Lower volatility = larger window\n",
    "            vol_ratio = current_vol / median_vol\n",
    "            window_size = int(base_window_size / vol_ratio)\n",
    "            \n",
    "            # Ensure window size stays within bounds\n",
    "            window_size = max(min_window_size, min(window_size, max_window_size))\n",
    "            \n",
    "            # Create window\n",
    "            if i + window_size <= len(self.df):\n",
    "                window = self.df.iloc[i:i + window_size].copy()\n",
    "                \n",
    "                # Only add windows with sufficient data\n",
    "                if len(window) >= min_window_size:\n",
    "                    # Add window index information\n",
    "                    window['window_start'] = i\n",
    "                    window['window_end'] = i + window_size\n",
    "                    window['window_id'] = window_id\n",
    "                    windows.append(window)\n",
    "                    window_id += 1\n",
    "            \n",
    "            # Move by step size (with overlap)\n",
    "            step_size = int(window_size * (1 - overlap_ratio))\n",
    "            i += max(1, step_size)  # Ensure we move at least 1 step\n",
    "        \n",
    "        self.windows = windows\n",
    "        \n",
    "        print(f\"Created {len(windows)} {'overlapping' if overlap_ratio > 0 else ''} windows\")\n",
    "        print(f\"Average window size: {sum(len(w) for w in windows) / len(windows):.2f}\")\n",
    "        print(f\"Min window size: {min(len(w) for w in windows)}\")\n",
    "        print(f\"Max window size: {max(len(w) for w in windows)}\")\n",
    "        \n",
    "        return windows\n",
    "\n",
    "\n",
    "    def analyze_windows(self):\n",
    "        \"\"\"\n",
    "        Print statistics about volatility in the windows\n",
    "        \"\"\"\n",
    "        if not self.windows:\n",
    "            raise ValueError(\"No windows created yet. Call create_regime_windows() or create_adaptive_windows() first.\")\n",
    "        \n",
    "        # First verify all windows have data\n",
    "        empty_windows = [i for i, w in enumerate(self.windows) if len(w) == 0]\n",
    "        if empty_windows:\n",
    "            raise ValueError(f\"Found empty windows at indices: {empty_windows}\")\n",
    "            \n",
    "        window_stats = []\n",
    "        for w in self.windows:\n",
    "            try:\n",
    "                stats = {\n",
    "                    'window_id': w['window_id'].iloc[0],\n",
    "                    'size': len(w),\n",
    "                    'mean_vol': w['Volatility'].mean(),\n",
    "                    'max_vol': w['Volatility'].max(),\n",
    "                    'min_vol': w['Volatility'].min(),\n",
    "                    'vol_range': w['Volatility'].max() - w['Volatility'].min(),\n",
    "                    'vol_stability': w['Volatility'].std() / w['Volatility'].mean(),\n",
    "                    'start_time': w['Timestamp'].iloc[0],\n",
    "                    'end_time': w['Timestamp'].iloc[-1]\n",
    "                }\n",
    "                \n",
    "                # Add regime_id if available\n",
    "                if 'regime_id' in w.columns:\n",
    "                    stats['regime_id'] = w['regime_id'].iloc[0]\n",
    "                    \n",
    "                window_stats.append(stats)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing window {w['window_id'].iloc[0] if len(w) > 0 else 'unknown'}:\")\n",
    "                print(f\"Window length: {len(w)}\")\n",
    "                print(f\"Error: {str(e)}\")\n",
    "                raise\n",
    "        \n",
    "        stats_df = pd.DataFrame(window_stats)\n",
    "        print(\"\\nWindow Statistics:\")\n",
    "        print(stats_df.describe())\n",
    "        return stats_df\n",
    "\n",
    "\n",
    "    \n",
    "    def _timestamp_to_seconds(self, ts):\n",
    "        \"\"\"Convert timestamp to seconds within the day\"\"\"\n",
    "        if isinstance(ts, str):\n",
    "            ts = pd.to_datetime(ts)\n",
    "        return ts.hour * 3600 + ts.minute * 60 + ts.second + ts.microsecond / 1e6\n",
    "    \n",
    "    def create_tensor_windows(self):\n",
    "        \"\"\"\n",
    "        Convert windows to tensor representation.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.RaggedTensor, dict\n",
    "            A ragged tensor of windows and metadata dictionary\n",
    "        \"\"\"\n",
    "        if not self.windows:\n",
    "            raise ValueError(\"No windows created yet. Call create_regime_windows() or create_adaptive_windows() first.\")\n",
    "        \n",
    "        # Create list to hold window arrays\n",
    "        window_arrays = []\n",
    "        window_lengths = []\n",
    "        window_metadata = []\n",
    "        \n",
    "        for window in self.windows:\n",
    "            # Convert timestamp to numerical feature\n",
    "            timestamps = pd.to_datetime(window['Timestamp'])\n",
    "            time_seconds = timestamps.apply(self._timestamp_to_seconds)\n",
    "            \n",
    "            # Ensure all features are properly shaped\n",
    "            features = np.column_stack([\n",
    "                time_seconds.values,  # Make sure it's a numpy array\n",
    "                window[self.feature_columns].values\n",
    "            ]).astype(np.float32)  # Ensure float32 dtype\n",
    "            \n",
    "            window_arrays.append(features)\n",
    "            window_lengths.append(len(features))\n",
    "            \n",
    "            # Store metadata\n",
    "            metadata = {\n",
    "                'window_id': window['window_id'].iloc[0],\n",
    "                'start_time': window['Timestamp'].iloc[0],\n",
    "                'end_time': window['Timestamp'].iloc[-1],\n",
    "                'mean_vol': window['Volatility'].mean()\n",
    "            }\n",
    "            \n",
    "            # Add regime_id if available\n",
    "            if 'regime_id' in window.columns:\n",
    "                metadata['regime_id'] = window['regime_id'].iloc[0]\n",
    "                \n",
    "            window_metadata.append(metadata)\n",
    "        \n",
    "        # Create ragged tensor with explicit shape\n",
    "        ragged_tensor = tf.RaggedTensor.from_row_lengths(\n",
    "            values=tf.concat(window_arrays, axis=0),\n",
    "            row_lengths=[len(w) for w in window_arrays]\n",
    "        )\n",
    "        \n",
    "        # Create dictionary with window information\n",
    "        window_info = {\n",
    "            'n_windows': len(self.windows),\n",
    "            'max_length': max(window_lengths),\n",
    "            'min_length': min(window_lengths),\n",
    "            'avg_length': sum(window_lengths) / len(window_lengths),\n",
    "            'feature_names': ['time_seconds'] + self.feature_columns,\n",
    "            'metadata': window_metadata,\n",
    "            'n_features': len(self.feature_columns) + 1  # Add 1 for time_seconds\n",
    "        }\n",
    "        \n",
    "        self.window_tensors = ragged_tensor\n",
    "        self.window_info = window_info\n",
    "        \n",
    "        print(f\"\\nTensor shape: {ragged_tensor.shape}\")\n",
    "        print(f\"Number of features: {window_info['n_features']}\")\n",
    "        print(f\"Max window length: {window_info['max_length']}\")\n",
    "        print(f\"Min window length: {window_info['min_length']}\")\n",
    "        print(f\"Average window length: {window_info['avg_length']:.2f}\")\n",
    "        \n",
    "        return ragged_tensor, window_info\n",
    "    \n",
    "    def pad_to_dense(self):\n",
    "        \"\"\"\n",
    "        Convert ragged tensor to dense tensor by padding with zeros\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        tf.Tensor\n",
    "            Padded dense tensor\n",
    "        \"\"\"\n",
    "        if self.window_tensors is None:\n",
    "            raise ValueError(\"No tensor windows created yet. Call create_tensor_windows() first.\")\n",
    "            \n",
    "        return self.window_tensors.to_tensor(default_value=0.0)\n",
    "    \n",
    "    def split_windows(self, train_ratio=0.7, val_ratio=0.15):\n",
    "        \"\"\"\n",
    "        Split window tensor into train/validation/test sets\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        train_ratio : float\n",
    "            Proportion of data for training\n",
    "        val_ratio : float\n",
    "            Proportion of data for validation\n",
    "            \n",
    "        Returns:\n",
    "        --------\n",
    "        tuple\n",
    "            (train_tensor, val_tensor, test_tensor)\n",
    "        \"\"\"\n",
    "        if self.window_tensors is None:\n",
    "            raise ValueError(\"No tensor windows created yet. Call create_tensor_windows() first.\")\n",
    "            \n",
    "        n_windows = self.window_tensors.shape[0]\n",
    "        indices = np.random.permutation(n_windows)\n",
    "        \n",
    "        train_size = int(n_windows * train_ratio)\n",
    "        val_size = int(n_windows * val_ratio)\n",
    "        \n",
    "        train_indices = indices[:train_size]\n",
    "        val_indices = indices[train_size:train_size + val_size]\n",
    "        test_indices = indices[train_size + val_size:]\n",
    "        \n",
    "        train_tensor = tf.gather(self.window_tensors, train_indices)\n",
    "        val_tensor = tf.gather(self.window_tensors, val_indices)\n",
    "        test_tensor = tf.gather(self.window_tensors, test_indices)\n",
    "        \n",
    "        print(f\"Training tensor shape: {train_tensor.shape}\")\n",
    "        print(f\"Validation tensor shape: {val_tensor.shape}\")\n",
    "        print(f\"Testing tensor shape: {test_tensor.shape}\")\n",
    "        \n",
    "        return train_tensor, val_tensor, test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected 762 volatility regimes\n",
      "Created 1884 windows from 762 regimes\n",
      "Average window size: 147.87\n",
      "Min window size: 40\n",
      "Max window size: 229\n",
      "\n",
      "Window Statistics:\n",
      "         window_id         size     mean_vol      max_vol      min_vol  \\\n",
      "count  1884.000000  1884.000000  1884.000000  1884.000000  1884.000000   \n",
      "mean    941.500000   147.868896     0.000334     0.000348     0.000320   \n",
      "std     544.008272    59.940387     0.000361     0.000371     0.000351   \n",
      "min       0.000000    40.000000     0.000018     0.000020     0.000014   \n",
      "25%     470.750000    88.000000     0.000232     0.000242     0.000221   \n",
      "50%     941.500000   180.500000     0.000270     0.000281     0.000259   \n",
      "75%    1412.250000   192.000000     0.000316     0.000329     0.000305   \n",
      "max    1883.000000   229.000000     0.003218     0.003225     0.003201   \n",
      "\n",
      "          vol_range  vol_stability    regime_id  \n",
      "count  1.884000e+03    1884.000000  1884.000000  \n",
      "mean   2.810576e-05       0.030617   429.266454  \n",
      "std    4.234299e-05       0.069112   220.937656  \n",
      "min    1.518991e-07       0.000147     0.000000  \n",
      "25%    8.833197e-06       0.009062   253.000000  \n",
      "50%    1.827969e-05       0.019727   459.000000  \n",
      "75%    3.256029e-05       0.034177   621.000000  \n",
      "max    7.226095e-04       1.848040   752.000000  \n",
      "\n",
      "Tensor shape: (1884, None, 4)\n",
      "Number of features: 4\n",
      "Max window length: 229\n",
      "Min window length: 40\n",
      "Average window length: 147.87\n",
      "Training tensor shape: (1318, None, 4)\n",
      "Validation tensor shape: (282, None, 4)\n",
      "Testing tensor shape: (284, None, 4)\n",
      "Features: ['time_seconds', 'Value', 'Volume', 'Volatility']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the window generator\n",
    "window_gen = VolatilityWindowGenerator(df)\n",
    "\n",
    "# Use regime-based windowing (better for handling different market conditions)\n",
    "windows = window_gen.create_regime_windows(sensitivity=1.8, min_regime_size=40)\n",
    "\n",
    "# Analyze the windows\n",
    "stats = window_gen.analyze_windows()\n",
    "\n",
    "# Convert to tensors\n",
    "tensors, info = window_gen.create_tensor_windows()\n",
    "\n",
    "# Split into train/val/test\n",
    "train_tensor, val_tensor, test_tensor = window_gen.split_windows()\n",
    "\n",
    "print(f\"Features: {info['feature_names']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Adaptations\n",
    "\n",
    "Modify the original CSDI code (GitHub) for denoising:\n",
    "\n",
    "### Remove Imputation Logic:\n",
    "- Delete code blocks that handle missing value imputation.\n",
    "\n",
    "### Time Embeddings:\n",
    "- Encode irregular timestamps as sinusoidal embeddings (normalized to [0,1]).\n",
    "\n",
    "### Conditioning Mechanism:\n",
    "- Use the noisy input as the conditional context (instead of partial observations).\n",
    "\n",
    "### Diffusion Process:\n",
    "- Use the original diffusion steps but disable masking (all positions are observed).\n",
    "- Adjust the noise schedule (Î²) to match financial noise characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training Pipeline\n",
    "\n",
    "**Objective:** Learn to reverse the diffusion process conditioned on noisy ticks.\n",
    "\n",
    "### Inputs:\n",
    "- **noisy_data:** Corrupted ticks (price, volume, etc.).\n",
    "- **mask:** All ones (no missing data).\n",
    "- **time_embeddings:** Encoded timestamps.\n",
    "\n",
    "### Forward Process:\n",
    "- Gradually add Gaussian noise to `noisy_data` across diffusion timesteps.\n",
    "- Forward Process: Replace Gaussian SDE with a market-realistic stochastic process.\n",
    "\n",
    "### Reverse Process:\n",
    "- Train the model to predict the score (gradient) to denoise the data.\n",
    "\n",
    "### Loss Function:\n",
    "- Weighted MSE between predicted and true noise at each diffusion step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Functions to Implement\n",
    "\n",
    "### Data Loader:\n",
    "```python\n",
    "def load_tick_data():\n",
    "    \"\"\"Reads raw ticks and converts to windowed sequences.\"\"\"\n",
    "    pass\n",
    "\n",
    "def inject_microstructure_noise():\n",
    "    \"\"\"Adds synthetic bid-ask bounce, order flow noise.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Time Embeddings:\n",
    "```python\n",
    "def encode_timestamps():\n",
    "    \"\"\"Converts irregular timestamps to continuous embeddings.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Diffusion Utils:\n",
    "```python\n",
    "def beta_scheduler():\n",
    "    \"\"\"Defines the noise schedule (linear, cosine, etc.).\"\"\"\n",
    "    pass\n",
    "\n",
    "def q_sample():\n",
    "    \"\"\"Forward diffusion process (adding noise).\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Model:\n",
    "```python\n",
    "class ConditionalScoreModel:\n",
    "    \"\"\"Modified CSDI backbone (transformer/TCN).\"\"\"\n",
    "    pass\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"Computes loss and updates weights.\"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Process\n",
    "\n",
    "### Hyperparameters:\n",
    "- Diffusion steps (`T=1000`), learning rate (`1e-4`), batch size (`64`).\n",
    "- Noise schedule (e.g., `beta_start=1e-4`, `beta_end=0.02`).\n",
    "\n",
    "### Training Loop:\n",
    "For each batch:\n",
    "1. Generate noisy data via `inject_microstructure_noise()`.\n",
    "2. Compute time embeddings for irregular timestamps.\n",
    "3. **Forward pass:** Corrupt noisy data with diffusion.\n",
    "4. **Reverse pass:** Predict denoised data.\n",
    "5. Update model weights via gradient descent.\n",
    "\n",
    "### Checkpointing:\n",
    "- Save model weights periodically (e.g., every epoch).\n",
    "- Track validation loss on a held-out tick dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Testing\n",
    "\n",
    "### Metrics:\n",
    "- **Reconstruction Loss:** MSE between denoised and clean data (if synthetic).\n",
    "- **Volatility Consistency:** Compare realized volatility of raw vs. denoised data.\n",
    "- **Microstructure Preservation:** Autocorrelation of trade signs.\n",
    "\n",
    "### Visual Checks:\n",
    "- Plot raw vs. denoised ticks (ensure no new timestamps are added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Pipeline\n",
    "\n",
    "### Preprocessing:\n",
    "- Normalize new tick data using the same scaler from training.\n",
    "- Encode timestamps.\n",
    "\n",
    "### Inference:\n",
    "- Run the trained CSDI in reverse diffusion mode with `mask=1`.\n",
    "\n",
    "### Postprocessing:\n",
    "- Inverse-transform normalized denoised data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
