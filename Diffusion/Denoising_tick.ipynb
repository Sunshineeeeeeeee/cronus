{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.Data Preprocessing\n",
    "\n",
    "### Cleaning + Estimating volatility\n",
    "- Ensuring validity of datapoints \n",
    "- Cleaning out deviations | Isolated points | etc ...\n",
    "- Estimating Volatility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpreprocess_td\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m preprocess_tick_data\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Would be lovely to estimate parameters of function\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m df_clean, df_diagnostics, outlier_counter \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocess_tick_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m df \u001b[38;5;241m=\u001b[39m df_clean\n\u001b[1;32m      6\u001b[0m df \u001b[38;5;241m=\u001b[39m df\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mVOLATILITY\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/code/scripts/Diffusion/preprocess_td.py:182\u001b[0m, in \u001b[0;36mpreprocess_tick_data\u001b[0;34m(df, symbol_col, timestamp_col, price_col, volume_col, window, z_threshold, min_price_change_pct)\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(lookback_window, \u001b[38;5;28mlen\u001b[39m(symbol_data) \u001b[38;5;241m-\u001b[39m lookahead_window):\n\u001b[1;32m    181\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m isolated_price\u001b[38;5;241m.\u001b[39miloc[i]:\n\u001b[0;32m--> 182\u001b[0m         current_price \u001b[38;5;241m=\u001b[39m \u001b[43msymbol_data\u001b[49m\u001b[43m[\u001b[49m\u001b[43mprice_col\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    184\u001b[0m         prev_stable \u001b[38;5;241m=\u001b[39m is_stable\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m-\u001b[39mlookback_window:i]\n\u001b[1;32m    185\u001b[0m         next_stable \u001b[38;5;241m=\u001b[39m is_stable\u001b[38;5;241m.\u001b[39miloc[i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m:i\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;241m+\u001b[39mlookahead_window]\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexing.py:1683\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1667\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_integer\u001b[39m(\u001b[38;5;28mself\u001b[39m, key: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m|\u001b[39m np\u001b[38;5;241m.\u001b[39minteger, axis: AxisInt) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1668\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1669\u001b[0m \u001b[38;5;124;03m    Check that 'key' is a valid position in the desired axis.\u001b[39;00m\n\u001b[1;32m   1670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;124;03m        If 'key' is not a valid position in axis 'axis'.\u001b[39;00m\n\u001b[1;32m   1682\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 1683\u001b[0m     len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1684\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[1;32m   1685\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/lib/python3.12/site-packages/pandas/core/indexes/base.py:909\u001b[0m, in \u001b[0;36mIndex.__len__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    900\u001b[0m         c\n\u001b[1;32m    901\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m c \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munique(level\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)[: get_option(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisplay.max_dir_items\u001b[39m\u001b[38;5;124m\"\u001b[39m)]\n\u001b[1;32m    902\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(c, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m c\u001b[38;5;241m.\u001b[39misidentifier()\n\u001b[1;32m    903\u001b[0m     }\n\u001b[1;32m    905\u001b[0m \u001b[38;5;66;03m# --------------------------------------------------------------------\u001b[39;00m\n\u001b[1;32m    906\u001b[0m \u001b[38;5;66;03m# Array-Like Methods\u001b[39;00m\n\u001b[1;32m    907\u001b[0m \n\u001b[1;32m    908\u001b[0m \u001b[38;5;66;03m# ndarray compat\u001b[39;00m\n\u001b[0;32m--> 909\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__len__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[1;32m    910\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    911\u001b[0m \u001b[38;5;124;03m    Return the length of the Index.\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m    913\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from preprocess_td import preprocess_tick_data\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    }
   ],
   "source": [
    "# Would be lovely to estimate parameters of function\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume  Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Synthetic Noise Injection \n",
    "\n",
    "Suppose that, the latent log-price Xt is an Ito-semimartingale of the form \n",
    "\n",
    "$dX_t = b_t dt + \\sigma_t dW_t + dJ_t,$  \n",
    "$d\\sigma_t = \\tilde{b}_t dt + \\tilde{\\sigma}^{(1)}_t dW_t + \\tilde{\\sigma}^{(2)}_t d\\tilde{W}_t + d\\tilde{J}_t$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Market Microstructure Noise:\n",
    "- Quote Spread Noise: The price fluctuation caused by trades alternating between bid and ask prices, creating a \"bouncing\" effect that obscures the true efficient price.\n",
    "\n",
    "- Order Flow Noise: Price movements driven by the imbalance between buy and sell orders, where persistent directional trading pressure can create temporary price trends away from the efficient price.\n",
    "\n",
    "- Strategic Order Noise: Price distortions created when large traders split their orders into smaller pieces to minimize market impact.\n",
    "\n",
    "- Quote Positioning Noise: Price effects from market makers strategically placing and canceling quotes to create false impressions of supply and demand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Simulating efficient price process...\n",
      "Adding quote spread noise...\n",
      "Simulating order flow noise...\n",
      "Simulating strategic order splitting...\n",
      "Simulating strategic quote positioning...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.03378606,  0.03567945,  0.09798677,  0.10495182, -0.12444308,\n",
       "       -0.01157954, -0.20460404,  0.04836754, -0.03279175,  0.04115321])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate 1000 noise samples\n",
    "from microstructure_simulator import MarketMicrostructureSimulator\n",
    "\n",
    "simulator = MarketMicrostructureSimulator(n_samples=1000)\n",
    "result = simulator.simulate_full_microstructure()\n",
    "noise_components = simulator.extract_noise_components(result)\n",
    "noise_components[\"total_noise\"][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting preprocessing with 570771 rows\n",
      "After filtering trading hours: 282810 rows\n",
      "After cleaning outliers: 282301 rows\n",
      "Final clean dataset: 278585 rows\n",
      "\n",
      "Outlier counts by detection method:\n",
      "  zscore: 64\n",
      "  extreme_deviation: 69\n",
      "  isolated_point: 390\n",
      "  price_reversal: 93\n",
      "  market_open_artifact: 0\n",
      "  timestamp_group: 34\n",
      "  price_velocity: 3703\n",
      "  suspicious_cluster: 52\n",
      "  wavelet_outlier: 24\n",
      "Estimating advanced tick-level volatility for 278585 ticks...\n",
      "Computing wavelet-based volatility for META.O...\n",
      "Completed advanced tick-level volatility estimation\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Timestamp</th>\n",
       "      <th>Value</th>\n",
       "      <th>Volume</th>\n",
       "      <th>Volatility</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.24</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.17</td>\n",
       "      <td>15.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.11</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-01-30 09:30:00.740000+00:00</td>\n",
       "      <td>694.10</td>\n",
       "      <td>249.0</td>\n",
       "      <td>0.000261</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                         Timestamp   Value  Volume  Volatility\n",
       "0 2025-01-30 09:30:00.740000+00:00  694.24    13.0    0.000260\n",
       "1 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000260\n",
       "2 2025-01-30 09:30:00.740000+00:00  694.17    15.0    0.000261\n",
       "3 2025-01-30 09:30:00.740000+00:00  694.11     8.0    0.000261\n",
       "4 2025-01-30 09:30:00.740000+00:00  694.10   249.0    0.000261"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "from preprocess_td import preprocess_tick_data\n",
    "from volatility_estimation import estimate_tick_volatility\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/Meta_Test.csv\")\n",
    "df = df.dropna()\n",
    "\n",
    "# Would be lovely to estimate parameters of function\n",
    "df_clean, df_diagnostics, outlier_counter = preprocess_tick_data(df)\n",
    "df = df_clean\n",
    "df = df.drop(columns=\"VOLATILITY\")\n",
    "\n",
    "df = estimate_tick_volatility(df, method = 'wavelet')\n",
    "\n",
    "df.drop(columns=['return', \"SYMBOL\"], inplace= True)\n",
    "df.rename(columns={'wavelet_vol' : 'Volatility', \n",
    "                  'TIMESTAMP':'Timestamp',\n",
    "                   'VALUE' : 'Value',\n",
    "                   'VOLUME' : 'Volume'}, inplace=True)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/Users/aleksandr/Desktop\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/my_data.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m df_sample \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m[:\u001b[38;5;241m2000\u001b[39m]\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# Save DataFrame to CSV\u001b[39;00m\n\u001b[1;32m      4\u001b[0m df_sample\u001b[38;5;241m.\u001b[39mto_csv(file_path, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "file_path = \"/Users/aleksandr/Desktop\" + \"/my_data.csv\"\n",
    "df_sample = df[:2000]\n",
    "# Save DataFrame to CSV\n",
    "df_sample.to_csv(file_path, index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv(\"/Users/aleksandr/Desktop/my_data.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2000, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[:2000]\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TDA Volatility Pipeline initialized\n",
      "\n",
      "Starting TDA volatility regime pipeline at 20:50:08\n",
      "\n",
      "Using first 1,700 observations for TDA analysis\n",
      "Extracting microstructure features with window sizes: [10, 50, 100]\n",
      "Computing microstructure features...\n",
      "Microstructure features computed in 0.01 seconds\n",
      "Computing order flow metrics...\n",
      "Order flow metrics computed in 0.00 seconds\n",
      "Computing DMI features...\n",
      "DMI features computed in 0.01 seconds\n",
      "Extracted 45 features from 1700 observations\n",
      "Feature extraction time: 0.05 seconds\n",
      "\n",
      "=== Using Progressive Feature Selection: 45 features ===\n",
      "Computing initial feature relevance scores...\n",
      "Initial scoring completed in 1.02 seconds\n",
      "Top 5 features by initial score:\n",
      "  adx_100: 0.1000\n",
      "  adx_50: 0.0941\n",
      "  sin_time: 0.0939\n",
      "  tr_100: 0.0939\n",
      "  tr_50: 0.0930\n",
      "First-stage selection: 30 candidates from 45 features\n",
      "Selected 30 features from 45\n",
      "Selected features: ['Value', 'seconds_from_open', 'sin_time', 'cos_time', 'trade_volume_profile_10', 'trade_frequency_10', 'tick_imbalance_50', 'trade_volume_profile_50', 'tick_imbalance_100', 'trade_volume_profile_100', 'order_flow_imbalance_10', 'order_flow_imbalance_50', 'order_flow_imbalance_100', 'price_impact_100', 'tr_10', 'adx_10', 'tr_50', 'plus_di_50', 'minus_di_50', 'adx_50', 'dmr_50', 'vol_adjusted_plus_di_50', 'vol_adjusted_minus_di_50', 'tr_100', 'plus_di_100', 'minus_di_100', 'adx_100', 'dmr_100', 'vol_adjusted_plus_di_100', 'vol_adjusted_minus_di_100']\n",
      "Enhancing features with information theory...\n",
      "Estimating Shannon entropy with heavy-tail adjustment...\n",
      "Shannon entropy estimation completed in 6.21 seconds\n",
      "Computing mutual information matrix using KDE...\n",
      "Mutual information computation completed in 48.21 seconds\n",
      "\n",
      "======================================================================\n",
      "COMPUTING TRANSFER ENTROPY: Analyzing causal information flow between variables\n",
      "======================================================================\n",
      "\n",
      "\n",
      "Transfer entropy computation completed in 0.93 seconds\n",
      "Maximum transfer entropy value: 0.2604\n",
      "======================================================================\n",
      "\n",
      "Feature enhancement took 55.34 seconds\n",
      "Enhanced features shape: (1700, 20)\n",
      "Feature enhancement took 55.34 seconds\n",
      "Enhanced features shape: (1700, 20)\n",
      "Enhanced feature names: ['adx_10', 'order_flow_imbalance_100', 'vol_adjusted_plus_di_50', 'plus_di_50', 'vol_adjusted_plus_di_100', 'plus_di_100', 'Value', 'sin_time', 'seconds_from_open', 'cos_time', 'adx_10_ent_weighted', 'order_flow_imbalance_100_ent_weighted', 'vol_adjusted_plus_di_50_ent_weighted', 'plus_di_50_ent_weighted', 'vol_adjusted_plus_di_100_ent_weighted', 'plus_di_100_ent_weighted', 'Value_ent_weighted', 'sin_time_ent_weighted', 'seconds_from_open_ent_weighted', 'cos_time_ent_weighted']\n",
      "Original MI matrix shape: (30, 30)\n",
      "Enhanced feature count: 20\n",
      "Found 10 matching features in original matrix\n",
      "Created transformed MI matrix of shape (20, 20)\n",
      "MI value range: [0.0000, 1.6962]\n",
      "Original TE matrix shape: (30, 30)\n",
      "Found 10 matching features in original TE matrix\n",
      "Created transformed TE matrix of shape (20, 20)\n",
      "TE value range: [0.0000, 0.1746]\n",
      "Computing temporally-weighted distance matrix...\n",
      "Successfully parsed string timestamps to datetime objects\n",
      "Applied MI-based feature weighting: min=0.6000, max=1.0000\n",
      "Applied TE-based feature weighting: min=1.0000, max=1.4074\n",
      "Distance matrix computation completed in 0.41 seconds\n",
      "Distance matrix shape: (1700, 1700)\n",
      "Distance range: [0.0000, 14.3841]\n",
      "Using Value as target for epsilon optimization\n",
      "\n",
      "=== Using Information-Theoretic Epsilon Optimization ===\n",
      "Using provided target feature: Value at index 6\n",
      "Epsilon 1.8952 (percentile 5.0%): Information gain = 0.2824\n",
      "Epsilon 2.5511 (percentile 11.6%): Information gain = 0.2525\n",
      "Epsilon 3.2070 (percentile 21.3%): Information gain = 0.2489\n",
      "Epsilon 3.8629 (percentile 33.1%): Information gain = 0.7855\n",
      "Epsilon 4.5188 (percentile 45.7%): Information gain = 8.0002\n",
      "Epsilon 5.1747 (percentile 58.9%): Information gain = 0.5518\n",
      "Epsilon 5.8306 (percentile 70.2%): Information gain = 1.3096\n",
      "Epsilon 6.4865 (percentile 79.4%): Information gain = 0.7600\n",
      "Epsilon 7.1424 (percentile 85.8%): Information gain = 0.5790\n",
      "Epsilon 7.7983 (percentile 90.0%): Information gain = 0.5466\n",
      "Optimal epsilon: 4.5188 (at 45.7 percentile)\n",
      "Information gain: 8.0002\n",
      "Recommended epsilon range: [3.3891, 6.7782]\n",
      "========================================================\n",
      "\n",
      "Computing persistent path zigzag homology for market microstructure...\n",
      "Window 1: Using adaptive epsilon range [3.3891, 1.9662]\n",
      "Window 2: Using adaptive epsilon range [3.3891, 1.5264]\n",
      "Window 3: Using adaptive epsilon range [3.3891, 1.4643]\n",
      "Window 4: Using adaptive epsilon range [3.3891, 2.1411]\n",
      "Window 5: Using adaptive epsilon range [3.3891, 2.2998]\n",
      "Window 6: Using adaptive epsilon range [3.3891, 1.4422]\n",
      "Window 7: Using adaptive epsilon range [3.3891, 2.2899]\n",
      "Window 8: Using adaptive epsilon range [3.3891, 2.0042]\n",
      "Window 9: Using adaptive epsilon range [3.3891, 1.6866]\n",
      "Window 10: Using adaptive epsilon range [3.3891, 1.7843]\n",
      "Window 11: Using adaptive epsilon range [3.3891, 2.6585]\n",
      "Window 12: Using adaptive epsilon range [3.3891, 2.2972]\n",
      "Window 13: Using adaptive epsilon range [3.3891, 1.7500]\n",
      "Window 14: Using adaptive epsilon range [3.3891, 2.2113]\n",
      "Window 15: Using adaptive epsilon range [3.3891, 2.5060]\n",
      "Window 16: Using adaptive epsilon range [3.3891, 2.1133]\n",
      "Window 17: Using adaptive epsilon range [3.3891, 2.0585]\n",
      "Window 18: Using adaptive epsilon range [3.3891, 2.0306]\n",
      "Window 19: Using adaptive epsilon range [3.3891, 1.9651]\n",
      "Window 20: Using adaptive epsilon range [3.3891, 2.3345]\n",
      "Window 21: Using adaptive epsilon range [3.3891, 2.8716]\n",
      "Window 22: Using adaptive epsilon range [3.3891, 3.1320]\n",
      "Window 23: Using adaptive epsilon range [3.3891, 2.4132]\n",
      "Window 24: Using adaptive epsilon range [3.3891, 2.0274]\n",
      "Window 25: Using adaptive epsilon range [3.3891, 1.6496]\n",
      "Window 26: Using adaptive epsilon range [3.3891, 1.3591]\n",
      "Window 27: Using adaptive epsilon range [3.3891, 1.7602]\n",
      "Window 28: Using adaptive epsilon range [3.3891, 3.6078]\n",
      "Window 29: Using adaptive epsilon range [3.3891, 3.3842]\n",
      "Window 30: Using adaptive epsilon range [3.3891, 4.1798]\n",
      "Window 31: Using adaptive epsilon range [3.3891, 2.3370]\n",
      "Window 32: Using adaptive epsilon range [3.3891, 1.7463]\n",
      "Window 33: Using adaptive epsilon range [3.3891, 2.0087]\n",
      "Network creation completed in 0.09 seconds\n",
      "Computing path complexes for 33 windows...\n",
      "Computing path complex for window 1/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 20907 paths in dimension 2 (0.00s)\n",
      "Window 1 path complex sizes: dim 0: 100, dim 1: 1456, dim 2: 20907\n",
      "Computing path complex for window 2/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 22485 paths in dimension 2 (0.00s)\n",
      "Window 2 path complex sizes: dim 0: 100, dim 1: 1566, dim 2: 22485\n",
      "Computing path complex for window 3/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24593 paths in dimension 2 (0.00s)\n",
      "Window 3 path complex sizes: dim 0: 100, dim 1: 1609, dim 2: 24593\n",
      "Computing path complex for window 4/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24267 paths in dimension 2 (0.00s)\n",
      "Window 4 path complex sizes: dim 0: 100, dim 1: 1568, dim 2: 24267\n",
      "Computing path complex for window 5/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24565 paths in dimension 2 (0.00s)\n",
      "Window 5 path complex sizes: dim 0: 100, dim 1: 1613, dim 2: 24565\n",
      "Computing path complex for window 6/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24095 paths in dimension 2 (0.00s)\n",
      "Window 6 path complex sizes: dim 0: 100, dim 1: 1667, dim 2: 24095\n",
      "Computing path complex for window 7/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24436 paths in dimension 2 (0.00s)\n",
      "Window 7 path complex sizes: dim 0: 100, dim 1: 1551, dim 2: 24436\n",
      "Computing path complex for window 8/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25559 paths in dimension 2 (0.00s)\n",
      "Window 8 path complex sizes: dim 0: 100, dim 1: 1630, dim 2: 25559\n",
      "Computing path complex for window 9/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 29554 paths in dimension 2 (0.00s)\n",
      "Window 9 path complex sizes: dim 0: 100, dim 1: 1767, dim 2: 29554\n",
      "Computing path complex for window 10/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25916 paths in dimension 2 (0.00s)\n",
      "Window 10 path complex sizes: dim 0: 100, dim 1: 1673, dim 2: 25916\n",
      "Computing path complex for window 11/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24140 paths in dimension 2 (0.00s)\n",
      "Window 11 path complex sizes: dim 0: 100, dim 1: 1665, dim 2: 24140\n",
      "Computing path complex for window 12/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24750 paths in dimension 2 (0.00s)\n",
      "Window 12 path complex sizes: dim 0: 100, dim 1: 1636, dim 2: 24750\n",
      "Computing path complex for window 13/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 22193 paths in dimension 2 (0.00s)\n",
      "Window 13 path complex sizes: dim 0: 100, dim 1: 1595, dim 2: 22193\n",
      "Computing path complex for window 14/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25564 paths in dimension 2 (0.00s)\n",
      "Window 14 path complex sizes: dim 0: 100, dim 1: 1662, dim 2: 25564\n",
      "Computing path complex for window 15/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25255 paths in dimension 2 (0.00s)\n",
      "Window 15 path complex sizes: dim 0: 100, dim 1: 1626, dim 2: 25255\n",
      "Computing path complex for window 16/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 27533 paths in dimension 2 (0.00s)\n",
      "Window 16 path complex sizes: dim 0: 100, dim 1: 1645, dim 2: 27533\n",
      "Computing path complex for window 17/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 22811 paths in dimension 2 (0.00s)\n",
      "Window 17 path complex sizes: dim 0: 100, dim 1: 1552, dim 2: 22811\n",
      "Computing path complex for window 18/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 24521 paths in dimension 2 (0.00s)\n",
      "Window 18 path complex sizes: dim 0: 100, dim 1: 1545, dim 2: 24521\n",
      "Computing path complex for window 19/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 29765 paths in dimension 2 (0.00s)\n",
      "Window 19 path complex sizes: dim 0: 100, dim 1: 1725, dim 2: 29765\n",
      "Computing path complex for window 20/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 29367 paths in dimension 2 (0.00s)\n",
      "Window 20 path complex sizes: dim 0: 100, dim 1: 1745, dim 2: 29367\n",
      "Computing path complex for window 21/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 28148 paths in dimension 2 (0.00s)\n",
      "Window 21 path complex sizes: dim 0: 100, dim 1: 1652, dim 2: 28148\n",
      "Computing path complex for window 22/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 26263 paths in dimension 2 (0.00s)\n",
      "Window 22 path complex sizes: dim 0: 100, dim 1: 1655, dim 2: 26263\n",
      "Computing path complex for window 23/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 29988 paths in dimension 2 (0.00s)\n",
      "Window 23 path complex sizes: dim 0: 100, dim 1: 1735, dim 2: 29988\n",
      "Computing path complex for window 24/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 27551 paths in dimension 2 (0.00s)\n",
      "Window 24 path complex sizes: dim 0: 100, dim 1: 1682, dim 2: 27551\n",
      "Computing path complex for window 25/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 28382 paths in dimension 2 (0.00s)\n",
      "Window 25 path complex sizes: dim 0: 100, dim 1: 1703, dim 2: 28382\n",
      "Computing path complex for window 26/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 20536 paths in dimension 2 (0.00s)\n",
      "Window 26 path complex sizes: dim 0: 100, dim 1: 1477, dim 2: 20536\n",
      "Computing path complex for window 27/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 20022 paths in dimension 2 (0.00s)\n",
      "Window 27 path complex sizes: dim 0: 100, dim 1: 1412, dim 2: 20022\n",
      "Computing path complex for window 28/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 27982 paths in dimension 2 (0.00s)\n",
      "Window 28 path complex sizes: dim 0: 100, dim 1: 1688, dim 2: 27982\n",
      "Computing path complex for window 29/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 31058 paths in dimension 2 (0.00s)\n",
      "Window 29 path complex sizes: dim 0: 100, dim 1: 1768, dim 2: 31058\n",
      "Computing path complex for window 30/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25010 paths in dimension 2 (0.00s)\n",
      "Window 30 path complex sizes: dim 0: 100, dim 1: 1654, dim 2: 25010\n",
      "Computing path complex for window 31/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 25030 paths in dimension 2 (0.00s)\n",
      "Window 31 path complex sizes: dim 0: 100, dim 1: 1633, dim 2: 25030\n",
      "Computing path complex for window 32/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 22750 paths in dimension 2 (0.00s)\n",
      "Window 32 path complex sizes: dim 0: 100, dim 1: 1599, dim 2: 22750\n",
      "Computing path complex for window 33/33...\n",
      "Computing path complex with max path length 2...\n",
      "Building dimension 2 paths...\n",
      "Created 28330 paths in dimension 2 (0.00s)\n",
      "Window 33 path complex sizes: dim 0: 100, dim 1: 1698, dim 2: 28330\n",
      "Processing homology in dimension 0...\n",
      "Dimension 0 processing completed in 0.12 seconds\n",
      "Processing homology in dimension 1...\n",
      "Dimension 1 processing completed in 0.20 seconds\n",
      "Processing homology in dimension 2...\n",
      "Dimension 2 processing completed in 168.21 seconds\n",
      "\n",
      "=== Persistence Analysis ===\n",
      "Found the following persistent features across windows:\n",
      "\n",
      "Dimension 0:\n",
      "- Persistent features found in 33 out of 33 windows\n",
      "- Average Betti number: 1.06\n",
      "- Maximum Betti number: 3\n",
      "- Betti number sequence: [3, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Dimension 1:\n",
      "- Persistent features found in 33 out of 33 windows\n",
      "- Average Betti number: 1434.55\n",
      "- Maximum Betti number: 1570\n",
      "- Betti number sequence: [1266, 1370, 1411, 1372, 1415, 1469, 1353, 1432, 1569, 1477, 1469, 1438, 1399, 1464, 1428, 1447, 1354, 1347, 1527, 1547, 1454, 1459, 1539, 1484, 1505, 1279, 1214, 1490, 1570, 1456, 1435, 1401, 1500]\n",
      "\n",
      "Dimension 2:\n",
      "- Persistent features found in 33 out of 33 windows\n",
      "- Average Betti number: 22296.61\n",
      "- Maximum Betti number: 27526\n",
      "- Betti number sequence: [18013, 19361, 21379, 21139, 21343, 20765, 21338, 22303, 26024, 22578, 20816, 21482, 19009, 22244, 22007, 24247, 19711, 21435, 26319, 25881, 24848, 22959, 26524, 24191, 24980, 17586, 17202, 24610, 27526, 21706, 21768, 19556, 24938]\n",
      "\n",
      "Persistence Stability Analysis:\n",
      "Dimension 0 stability: 96.88%\n",
      "Dimension 1 stability: 0.00%\n",
      "Dimension 2 stability: 0.00%\n",
      "===========================\n",
      "\n",
      "Computing transitions between windows...\n",
      "Total path zigzag persistence computation completed in 169.40 seconds\n",
      "Identifying 4 volatility regimes...\n",
      "\n",
      "=== Extracting Topological Features ===\n",
      "Extracting topological features...\n",
      "Using existing zigzag persistence computations...\n",
      "Extracting features from zigzag persistence...\n",
      "Extracting network statistics as topological features...\n",
      "Extracted 28 topological features in 0.00 seconds\n",
      " - 0 persistence diagram features\n",
      " - 23 zigzag persistence features\n",
      " - 5 network statistics features\n",
      "Extracted 28 topological features:\n",
      "  1. zigzag_dim0_betti_mean: 1.0606\n",
      "  2. zigzag_dim0_betti_max: 3.0000\n",
      "  3. zigzag_dim0_betti_min: 1.0000\n",
      "  4. zigzag_dim0_betti_std: 0.3428\n",
      "  5. zigzag_dim0_betti_range: 2.0000\n",
      "  6. zigzag_dim1_betti_mean: 1434.5455\n",
      "  7. zigzag_dim1_betti_max: 1570.0000\n",
      "  8. zigzag_dim1_betti_min: 1214.0000\n",
      "  9. zigzag_dim1_betti_std: 82.2863\n",
      "  10. zigzag_dim1_betti_range: 356.0000\n",
      "  11. zigzag_dim2_betti_mean: 22296.6061\n",
      "  12. zigzag_dim2_betti_max: 27526.0000\n",
      "  13. zigzag_dim2_betti_min: 17202.0000\n",
      "  14. zigzag_dim2_betti_std: 2656.4606\n",
      "  15. zigzag_dim2_betti_range: 10324.0000\n",
      "  16. zigzag_dim1_transition_mean: 1002.9375\n",
      "  17. zigzag_dim1_transition_max: 1222.0000\n",
      "  18. zigzag_dim1_transition_total: 32094.0000\n",
      "  19. zigzag_dim1_transition_std: 111.8190\n",
      "  20. zigzag_dim2_transition_mean: 18148.8750\n",
      "  21. zigzag_dim2_transition_max: 24194.0000\n",
      "  22. zigzag_dim2_transition_total: 580764.0000\n",
      "  23. zigzag_dim2_transition_std: 2483.5739\n",
      "  24. network_nodes_mean: 100.0000\n",
      "  25. network_edges_mean: 1631.8788\n",
      "  26. network_density_mean: 0.1648\n",
      "  27. network_degree_mean: 32.6376\n",
      "  28. network_degree_max: 73.0000\n",
      "======================================\n",
      "\n",
      "Combined feature matrix shape: (1700, 48)\n",
      "Affinity matrix range: [0.0000, 1.0000]\n",
      "Spectral clustering failed: Imbalanced clustering detected\n",
      "Falling back to robust GMM clustering...\n",
      "Final regime distribution: [162 948 258 332]\n",
      "Regime balance ratio: 0.171\n",
      "Regime identification completed in 10.21 seconds\n",
      "Identified 4 volatility regimes\n",
      "\n",
      "Training regime extension model...\n",
      "\n",
      "--- STEP 4: Training XGBoost Model for Regime Extension ---\n",
      "Preparing training data...\n",
      "Training set size: 1360, Validation set size: 340\n",
      "Number of features: 20\n",
      "Number of regimes: 4\n",
      "\n",
      "XGBoost parameters:\n",
      "  objective: multi:softprob\n",
      "  num_class: 4\n",
      "  max_depth: 6\n",
      "  learning_rate: 0.1\n",
      "  subsample: 0.8\n",
      "  colsample_bytree: 0.8\n",
      "  tree_method: hist\n",
      "  eval_metric: ['mlogloss', 'merror']\n",
      "  verbosity: 1\n",
      "  model_type: xgboost\n",
      "  train_size: 0.8\n",
      "  timestamps: ['2025-01-30 09:30:00.740000+00:00' '2025-01-30 09:30:00.740000+00:00'\n",
      " '2025-01-30 09:30:00.740000+00:00' ... '2025-01-30 09:52:55.563000+00:00'\n",
      " '2025-01-30 09:52:56.186000+00:00' '2025-01-30 09:52:57.449000+00:00']\n",
      "\n",
      "Creating DMatrix objects...\n",
      "\n",
      "Starting XGBoost training...\n",
      "[0]\ttrain-mlogloss:1.20597\ttrain-merror:0.02353\tval-mlogloss:1.30810\tval-merror:0.45000\n",
      "[10]\ttrain-mlogloss:0.39932\ttrain-merror:0.00882\tval-mlogloss:0.71879\tval-merror:0.14118\n",
      "[20]\ttrain-mlogloss:0.15713\ttrain-merror:0.00368\tval-mlogloss:0.58307\tval-merror:0.12941\n",
      "[30]\ttrain-mlogloss:0.06815\ttrain-merror:0.00000\tval-mlogloss:0.55481\tval-merror:0.13235\n",
      "[32]\ttrain-mlogloss:0.05842\ttrain-merror:0.00074\tval-mlogloss:0.55298\tval-merror:0.13235\n",
      "\n",
      "Final model performance:\n",
      "Training accuracy: 0.9993\n",
      "Validation accuracy: 0.8676\n",
      "\n",
      "XGBoost model training completed\n",
      "\n",
      "--- STEP 5: Extending Regimes to Remaining 300 Observations ---\n",
      "Processing in 3 batches of size 100\n",
      "\n",
      "Processing batch 1/3\n",
      "Batch range: 0 to 100 (100 observations)\n",
      "Extracting features from batch with 100 observations...\n",
      "Computing microstructure features...\n",
      "Microstructure features computed in 0.01 seconds\n",
      "Computing order flow metrics...\n",
      "Order flow metrics computed in 0.01 seconds\n",
      "Computing DMI features...\n",
      "DMI features computed in 0.01 seconds\n",
      "Extracted 45 features from batch\n",
      "Feature count: 45\n",
      "Using adaptive enhancement\n",
      "Creating batch-optimized feature set...\n",
      "Selected 20 features based on original training features\n",
      "Batch completed in 0.04 seconds (2631.1 samples/sec)\n",
      "Batch regime distribution: [ 6 80 12  2]\n",
      "\n",
      "Processing batch 2/3\n",
      "Batch range: 100 to 200 (100 observations)\n",
      "Extracting features from batch with 100 observations...\n",
      "Computing microstructure features...\n",
      "Microstructure features computed in 0.00 seconds\n",
      "Computing order flow metrics...\n",
      "Order flow metrics computed in 0.00 seconds\n",
      "Computing DMI features...\n",
      "DMI features computed in 0.01 seconds\n",
      "Extracted 45 features from batch\n",
      "Feature count: 45\n",
      "Using adaptive enhancement\n",
      "Creating batch-optimized feature set...\n",
      "Selected 20 features based on original training features\n",
      "Batch completed in 0.03 seconds (3795.9 samples/sec)\n",
      "Batch regime distribution: [ 0 76 24]\n",
      "\n",
      "Processing batch 3/3\n",
      "Batch range: 200 to 300 (100 observations)\n",
      "Extracting features from batch with 100 observations...\n",
      "Computing microstructure features...\n",
      "Microstructure features computed in 0.00 seconds\n",
      "Computing order flow metrics...\n",
      "Order flow metrics computed in 0.00 seconds\n",
      "Computing DMI features...\n",
      "DMI features computed in 0.01 seconds\n",
      "Extracted 45 features from batch\n",
      "Feature count: 45\n",
      "Using adaptive enhancement\n",
      "Creating batch-optimized feature set...\n",
      "Selected 20 features based on original training features\n",
      "Batch completed in 0.03 seconds (3984.5 samples/sec)\n",
      "Batch regime distribution: [ 5 75 20]\n",
      "Extension completed in 0.09 seconds\n",
      "Final regime distribution: [ 11 231  56   2]\n",
      "\n",
      "TDA volatility regime pipeline completed in 236.97 seconds\n",
      "Processed 2,000 total observations at 8.4 samples/sec\n"
     ]
    }
   ],
   "source": [
    "from volatility_regimes_identification.tda_volatility_pipeline import TDAVolatilityPipeline\n",
    "\n",
    "pipeline = TDAVolatilityPipeline()\n",
    "\n",
    "# Process entire dataset\n",
    "result_df = pipeline.process_data(\n",
    "    df,\n",
    "    timestamp_col='Timestamp',\n",
    "    price_col='Value',\n",
    "    volume_col='Volume',\n",
    "    volatility_col='Volatility',\n",
    "    n_regimes=4,\n",
    "    training_batch=1700,  \n",
    "    batch_size=100       \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 6)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequential regime counts:\n",
      "\n",
      "Regime 1.0:\n",
      "Number of sequences: 30\n",
      "Sequence lengths: [10, 30, 1, 4, 465, 119, 1, 1, 125, 5, 5, 37, 1, 128, 10, 2, 3, 1, 2, 1, 2, 7, 3, 65, 2, 2, 1, 71, 11, 64]\n",
      "Average sequence length: 39.30\n",
      "\n",
      "Regime 0.0:\n",
      "Number of sequences: 10\n",
      "Sequence lengths: [34, 1, 2, 10, 49, 66, 2, 3, 1, 5]\n",
      "Average sequence length: 17.30\n",
      "\n",
      "Regime 3.0:\n",
      "Number of sequences: 13\n",
      "Sequence lengths: [24, 19, 33, 3, 1, 28, 87, 46, 1, 23, 67, 1, 1]\n",
      "Average sequence length: 25.69\n",
      "\n",
      "Regime 2.0:\n",
      "Number of sequences: 10\n",
      "Sequence lengths: [30, 22, 56, 159, 3, 1, 1, 2, 20, 20]\n",
      "Average sequence length: 31.40\n"
     ]
    }
   ],
   "source": [
    "# Get the regime values as a list\n",
    "regimes = result_df[\"regime\"].tolist()\n",
    "\n",
    "# Initialize counters\n",
    "current_regime = regimes[0]\n",
    "current_count = 1\n",
    "regime_sequences = {}\n",
    "\n",
    "# Count sequential regimes\n",
    "for i in range(1, len(regimes)):\n",
    "    if regimes[i] == current_regime:\n",
    "        current_count += 1\n",
    "    else:\n",
    "        # Store the sequence count\n",
    "        key = f\"Regime {current_regime}\"\n",
    "        if key not in regime_sequences:\n",
    "            regime_sequences[key] = []\n",
    "        regime_sequences[key].append(current_count)\n",
    "        \n",
    "        # Reset counters\n",
    "        current_regime = regimes[i]\n",
    "        current_count = 1\n",
    "\n",
    "# Add the last sequence\n",
    "key = f\"Regime {current_regime}\"\n",
    "if key not in regime_sequences:\n",
    "    regime_sequences[key] = []\n",
    "regime_sequences[key].append(current_count)\n",
    "\n",
    "# Print results\n",
    "print(\"Sequential regime counts:\")\n",
    "for regime, sequences in regime_sequences.items():\n",
    "    print(f\"\\n{regime}:\")\n",
    "    print(f\"Number of sequences: {len(sequences)}\")\n",
    "    print(f\"Sequence lengths: {sequences}\")\n",
    "    print(f\"Average sequence length: {sum(sequences)/len(sequences):.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model Architecture Adaptations\n",
    "\n",
    "Modify the original CSDI code (GitHub) for denoising:\n",
    "\n",
    "### Remove Imputation Logic:\n",
    "- Delete code blocks that handle missing value imputation.\n",
    "\n",
    "### Time Embeddings:\n",
    "- Encode irregular timestamps as sinusoidal embeddings (normalized to [0,1]).\n",
    "\n",
    "### Conditioning Mechanism:\n",
    "- Use the noisy input as the conditional context (instead of partial observations).\n",
    "\n",
    "### Diffusion Process:\n",
    "- Use the original diffusion steps but disable masking (all positions are observed).\n",
    "- Adjust the noise schedule () to match financial noise characteristics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 4. Training Pipeline\n",
    "\n",
    "**Objective:** Learn to reverse the diffusion process conditioned on noisy ticks.\n",
    "\n",
    "### Inputs:\n",
    "- **noisy_data:** Corrupted ticks (price, volume, etc.).\n",
    "- **mask:** All ones (no missing data).\n",
    "- **time_embeddings:** Encoded timestamps.\n",
    "\n",
    "### Forward Process:\n",
    "- Gradually add Gaussian noise to `noisy_data` across diffusion timesteps.\n",
    "- Forward Process: Replace Gaussian SDE with a market-realistic stochastic process.\n",
    "\n",
    "### Reverse Process:\n",
    "- Train the model to predict the score (gradient) to denoise the data.\n",
    "\n",
    "### Loss Function:\n",
    "- Weighted MSE between predicted and true noise at each diffusion step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Functions to Implement\n",
    "\n",
    "### Data Loader:\n",
    "```python\n",
    "def load_tick_data():\n",
    "    \"\"\"Reads raw ticks and converts to windowed sequences.\"\"\"\n",
    "    pass\n",
    "\n",
    "def inject_microstructure_noise():\n",
    "    \"\"\"Adds synthetic bid-ask bounce, order flow noise.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Time Embeddings:\n",
    "```python\n",
    "def encode_timestamps():\n",
    "    \"\"\"Converts irregular timestamps to continuous embeddings.\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Diffusion Utils:\n",
    "```python\n",
    "def beta_scheduler():\n",
    "    \"\"\"Defines the noise schedule (linear, cosine, etc.).\"\"\"\n",
    "    pass\n",
    "\n",
    "def q_sample():\n",
    "    \"\"\"Forward diffusion process (adding noise).\"\"\"\n",
    "    pass\n",
    "```\n",
    "\n",
    "### Model:\n",
    "```python\n",
    "class ConditionalScoreModel:\n",
    "    \"\"\"Modified CSDI backbone (transformer/TCN).\"\"\"\n",
    "    pass\n",
    "\n",
    "def train_step():\n",
    "    \"\"\"Computes loss and updates weights.\"\"\"\n",
    "    pass\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training Process\n",
    "\n",
    "### Hyperparameters:\n",
    "- Diffusion steps (`T=1000`), learning rate (`1e-4`), batch size (`64`).\n",
    "- Noise schedule (e.g., `beta_start=1e-4`, `beta_end=0.02`).\n",
    "\n",
    "### Training Loop:\n",
    "For each batch:\n",
    "1. Generate noisy data via `inject_microstructure_noise()`.\n",
    "2. Compute time embeddings for irregular timestamps.\n",
    "3. **Forward pass:** Corrupt noisy data with diffusion.\n",
    "4. **Reverse pass:** Predict denoised data.\n",
    "5. Update model weights via gradient descent.\n",
    "\n",
    "### Checkpointing:\n",
    "- Save model weights periodically (e.g., every epoch).\n",
    "- Track validation loss on a held-out tick dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Validation & Testing\n",
    "\n",
    "### Metrics:\n",
    "- **Reconstruction Loss:** MSE between denoised and clean data (if synthetic).\n",
    "- **Volatility Consistency:** Compare realized volatility of raw vs. denoised data.\n",
    "- **Microstructure Preservation:** Autocorrelation of trade signs.\n",
    "\n",
    "### Visual Checks:\n",
    "- Plot raw vs. denoised ticks (ensure no new timestamps are added)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Deployment Pipeline\n",
    "\n",
    "### Preprocessing:\n",
    "- Normalize new tick data using the same scaler from training.\n",
    "- Encode timestamps.\n",
    "\n",
    "### Inference:\n",
    "- Run the trained CSDI in reverse diffusion mode with `mask=1`.\n",
    "\n",
    "### Postprocessing:\n",
    "- Inverse-transform normalized denoised data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
